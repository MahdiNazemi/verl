defaults:
  - ppo_trainer
  - _self_


algorithm:
  adv_estimator: grpo

data:
  train_batch_size: 64
  val_batch_size: 512
  max_prompt_length: 1024
  max_response_length: 8192
  filter_overlong_prompts: True
  seed: 42

custom_reward_function:
  path: ./verl/trainer/ppo/math_grader.py
  name: compute_math_score

actor_rollout_ref:
  model:
    path: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
    use_remove_padding: True
    use_liger: False
    use_fused_kernels: False
    fused_kernel_options:
      impl_backend: torch
    enable_gradient_checkpointing: True
  actor:
    ppo_mini_batch_size: 32
    use_dynamic_bsz: True
    ppo_max_token_len_per_gpu: 32768
    clip_ratio_high: 0.28
    use_kl_loss: False
    entropy_coeff: 0.000
    kl_loss_coef: 0.000
    kl_loss_type: low_var_kl
    ulysses_sequence_parallel_size: 1
    loss_agg_mode: seq-mean-token-sum-norm
    optim:
      weight_decay: 0.0
      betas: [0.9, 0.95]
    fsdp_config:
      dtype: float16
      param_offload: False
      optimizer_offload: True
  rollout:
    name: vllm
    dtype: float16
    enforce_eager: False
    free_cache_engine: True
    enable_chunked_prefill: True
    calculate_log_probs: True
    response_length: ${data.max_response_length}
    max_num_batched_tokens: 20480
    tensor_model_parallel_size: 1
    gpu_memory_utilization: 0.90
    n: 16
    temperature: 1.0
    val_kwargs:
      temperature: 0.6
      n: 16
      do_sample: True

trainer:
  project_name: 'fp16_is_all_you_need'
  experiment_name: 'math_1460-fp16-ppo'
  logger: ['console','wandb']
  val_before_train: True
  nnodes: 1
  n_gpus_per_node: 8
  save_freq: 200
  test_freq: 50
  log_val_generations: 20
  max_actor_ckpt_to_keep: 8
  total_epochs: 1
